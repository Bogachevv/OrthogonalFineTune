{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-09T19:22:58.969348Z","iopub.status.busy":"2024-09-09T19:22:58.968846Z","iopub.status.idle":"2024-09-09T19:22:58.980055Z","shell.execute_reply":"2024-09-09T19:22:58.978894Z","shell.execute_reply.started":"2024-09-09T19:22:58.969287Z"},"trusted":true},"outputs":[],"source":["from IPython.display import clear_output"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-09T19:22:59.143657Z","iopub.status.busy":"2024-09-09T19:22:59.143244Z","iopub.status.idle":"2024-09-09T19:23:35.643472Z","shell.execute_reply":"2024-09-09T19:23:35.642029Z","shell.execute_reply.started":"2024-09-09T19:22:59.143617Z"},"trusted":true},"outputs":[],"source":["!pip install transformers peft datasets evaluate huggingface_hub trl omegaconf --upgrade\n","clear_output()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-09T19:23:35.646779Z","iopub.status.busy":"2024-09-09T19:23:35.646233Z","iopub.status.idle":"2024-09-09T19:23:35.666972Z","shell.execute_reply":"2024-09-09T19:23:35.665833Z","shell.execute_reply.started":"2024-09-09T19:23:35.646708Z"},"trusted":true},"outputs":[],"source":["from kaggle_secrets import UserSecretsClient\n","import os\n","api_keys = UserSecretsClient()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-09T19:23:35.668932Z","iopub.status.busy":"2024-09-09T19:23:35.668577Z","iopub.status.idle":"2024-09-09T19:23:39.173216Z","shell.execute_reply":"2024-09-09T19:23:39.171895Z","shell.execute_reply.started":"2024-09-09T19:23:35.668893Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n","wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["0"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["os.system(f'wandb login {api_keys.get_secret(\"wandb\")}')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-09T19:23:39.177051Z","iopub.status.busy":"2024-09-09T19:23:39.176542Z","iopub.status.idle":"2024-09-09T19:23:40.935949Z","shell.execute_reply":"2024-09-09T19:23:40.934704Z","shell.execute_reply.started":"2024-09-09T19:23:39.176996Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: read).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]},{"data":{"text/plain":["0"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["os.system(f'huggingface-cli login --token {api_keys.get_secret(\"huggingface-cli\")}')"]},{"cell_type":"code","execution_count":7,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-09T19:27:52.892234Z","iopub.status.busy":"2024-09-09T19:27:52.891709Z","iopub.status.idle":"2024-09-09T19:28:18.546252Z","shell.execute_reply":"2024-09-09T19:28:18.545015Z","shell.execute_reply.started":"2024-09-09T19:27:52.892189Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import scipy\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","import transformers\n","from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM, AutoModelForSeq2SeqLM, BertTokenizer, BertModel \n","from transformers import TrainingArguments, Trainer, Seq2SeqTrainingArguments, Seq2SeqTrainer, GenerationConfig, DataCollatorWithPadding\n","from transformers import pipeline\n","from peft import BOFTConfig, get_peft_model, LoraConfig, TaskType\n","from datasets import load_dataset\n","import evaluate\n","from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n","\n","import wandb\n","from omegaconf import OmegaConf\n","\n","# from deepeval.benchmarks import MMLU\n","# from deepeval.benchmarks.tasks import MMLUTask\n","# from deepeval.models.base_model import DeepEvalBaseLLM\n","\n","import pickle\n","import tqdm.notebook as tqdm\n","\n","clear_output()"]},{"cell_type":"markdown","metadata":{},"source":["# Massive multitask language understanding (MMLU benchmark)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-09T19:28:18.549131Z","iopub.status.busy":"2024-09-09T19:28:18.548592Z","iopub.status.idle":"2024-09-09T19:28:20.154319Z","shell.execute_reply":"2024-09-09T19:28:20.152543Z","shell.execute_reply.started":"2024-09-09T19:28:18.549053Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-09-09 19:28:19--  https://raw.githubusercontent.com/hendrycks/test/master/categories.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2563 (2.5K) [text/plain]\n","Saving to: 'categories.py'\n","\n","categories.py       100%[===================>]   2.50K  --.-KB/s    in 0s      \n","\n","2024-09-09 19:28:20 (33.8 MB/s) - 'categories.py' saved [2563/2563]\n","\n"]}],"source":["# Loading MMLU categories\n","if not os.path.exists('./categories.py'):\n","    !wget https://raw.githubusercontent.com/hendrycks/test/master/categories.py\n","\n","from categories import subcategories, categories as categories_inv"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-09T19:28:20.156659Z","iopub.status.busy":"2024-09-09T19:28:20.156268Z","iopub.status.idle":"2024-09-09T19:28:20.164251Z","shell.execute_reply":"2024-09-09T19:28:20.162659Z","shell.execute_reply.started":"2024-09-09T19:28:20.156620Z"},"trusted":true},"outputs":[],"source":["for subcat_name, cat_names in subcategories.items():\n","    subcategories[subcat_name] = cat_names[0] if isinstance(cat_names, list) else cat_names\n","    \n","categories = {}\n","\n","for cat_name, subcats in categories_inv.items():\n","    for subcat in subcats:\n","        categories[subcat] = cat_name"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-09T19:28:20.167153Z","iopub.status.busy":"2024-09-09T19:28:20.166711Z","iopub.status.idle":"2024-09-09T19:28:20.178535Z","shell.execute_reply":"2024-09-09T19:28:20.177258Z","shell.execute_reply.started":"2024-09-09T19:28:20.167103Z"},"trusted":true},"outputs":[],"source":["def subcat_to_cat(subcat):\n","    cat_name = subcategories[subcat]\n","    cat_name = categories[cat_name]\n","    \n","    return cat_name"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-09T19:28:20.183089Z","iopub.status.busy":"2024-09-09T19:28:20.182023Z","iopub.status.idle":"2024-09-09T19:28:20.200149Z","shell.execute_reply":"2024-09-09T19:28:20.198498Z","shell.execute_reply.started":"2024-09-09T19:28:20.182991Z"},"trusted":true},"outputs":[],"source":["config = OmegaConf.create({\n","    'model_name':   'meta-llama/Meta-Llama-3-8B-Instruct',\n","    'padding_side': 'left',\n","    'task_name':    'all',\n","    'max_length':   256,\n","    'n_shots': 2,\n","    'fp16': True,\n","    'bf16': False,\n","    'LoRA_config': {\n","        'r': 16, \n","        'lora_alpha': 32, \n","        'lora_dropout': 0.05,\n","        'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n","    },\n","    'evaluation_config':{\n","        'num_splits': 20,\n","        'max_new_tokens': 4,\n","        'batch_size': 1,\n","        'empty_cache': True,\n","    },\n","    'trainer_config': {\n","        'output_dir': \"bogachevv/Llama-3-8b-MMLU\",\n","        'max_seq_length': 512,\n","        'dataset_text_field': 'text',\n","        'fp16': True,\n","        'full_determinism': False,\n","        'per_device_train_batch_size': 1,\n","        'per_device_eval_batch_size':  1,\n","        'gradient_accumulation_steps': 8,\n","        'lr_scheduler_type': 'cosine_with_restarts',\n","        'lr_scheduler_kwargs':{\n","            'num_cycles': 6,\n","        },\n","        'warmup_steps': 100,\n","#         'num_train_epochs': 2,\n","        'learning_rate': 1e-4,\n","        'max_steps': 2048,\n","        'weight_decay': 0.01,\n","#         'warmup_ratio': 1e-2,\n","        'dataloader_num_workers': 2,\n","        'eval_strategy': \"steps\",\n","#         'torch_empty_cache_steps': 16,\n","        'eval_steps': 16,\n","        'logging_steps': 16,\n","        'load_best_model_at_end': True,\n","        'seed': 42,\n","        'data_seed': 42,\n","        'report_to': 'wandb',\n","#         'predict_with_generate': True,\n","#         'push_to_hub': True,\n","#         'hub_model_id': 'LLama-LoRA-test',\n","#         'hub_strategy': 'checkpoint',\n","#         'save_strategy': \"steps\",\n","        'save_steps': 128,\n","    },\n","})"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-09T19:28:20.202411Z","iopub.status.busy":"2024-09-09T19:28:20.201826Z","iopub.status.idle":"2024-09-09T19:28:20.220481Z","shell.execute_reply":"2024-09-09T19:28:20.219136Z","shell.execute_reply.started":"2024-09-09T19:28:20.202344Z"},"trusted":true},"outputs":[{"data":{"text/plain":["omegaconf.dictconfig.DictConfig"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["type(config)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\n","    config.model_name, \n","    padding_side=config.padding_side,\n","#     model_max_length=512,\n",")\n","tokenizer.pad_token = tokenizer.eos_token\n","EOS_TOKEN = tokenizer.eos_token\n","\n","mmlu_dataset =  load_dataset(\"cais/mmlu\", config.task_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Try block for clear_output() call iff succes\n","try:\n","    few_shot_datasets = {\n","        subject: mmlu_dataset['dev'].filter(lambda row: row['subject'] == subject)\n","        for subject in set(mmlu_dataset['dev']['subject'])\n","    }\n","    \n","    clear_output()\n","    print('Succes')\n","except:\n","    raise"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def prepare_question(examples):\n","    prompt = f\"{examples['question']}\\n\"\n","    for letter, choice in zip(('A', 'B', 'C', 'D'), examples['choices']):\n","        prompt += f\"{letter}. {choice}\\n\"\n","\n","    answer = chr(65 + examples['answer'])\n","    \n","    return prompt, answer\n","\n","def prepare_prompt(examples, dev_dataset = None):\n","    if dev_dataset:\n","        yield from map(prepare_question, dev_dataset)\n","    \n","    yield prepare_question(examples)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def prepare_instruction_text(example):\n","    instructions = [\n","        {\"role\": \"system\", \"content\": f\"The following are multiple choice questions (with answers) about {example['subject']}. Output 'A', 'B', 'C', or 'D'. Full answer not needed.\"},\n","    ]\n","\n","    if config['n_shots'] and example['subject']:\n","        few_shot_dataset = few_shot_datasets[example['subject']]\n","        few_shot_dataset = few_shot_dataset.select(range(config['n_shots']))\n","    else:\n","        few_shot_dataset = None\n","    \n","    for prompt, ans in prepare_prompt(example, dev_dataset=few_shot_dataset):\n","        instructions.append({\"role\": \"user\", \"content\": prompt})\n","        instructions.append({\"role\": \"assistant\", \"content\": ans})\n","    \n","    text = tokenizer.apply_chat_template(\n","        instructions,\n","        tokenize=False\n","    )\n","    \n","    return {'text': text}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def r_replace(line, old, new):\n","    return line[::-1].replace(old[::-1], new[::-1], 1)[::-1]\n","\n","def remove_answer(example):\n","    text_wa_answer = example['text']\n","    text_wa_answer = text_wa_answer.rsplit('<|eot_id|>', 1)[0][:-1]\n","    \n","    # for letter in ('A', 'B', 'C', 'D'):\n","        # text_wa_answer = text_wa_answer.replace(f'<|start_header_id|>assistant<|end_header_id|>\\n\\n{letter}<|eot_id|>', '<|start_header_id|>assistant<|end_header_id|>\\n\\n')\n","        # text_wa_answer = r_replace(text_wa_answer, f'<|start_header_id|>assistant<|end_header_id|>\\n\\n{letter}<|eot_id|>', '<|start_header_id|>assistant<|end_header_id|>\\n\\n')\n","    \n","    return {'text_wa_answer': text_wa_answer}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["instructions_datasets = mmlu_dataset.map(prepare_instruction_text, batched=False, num_proc=2)\n","instructions_datasets['validation'] = instructions_datasets['validation'].map(remove_answer, batched=False)\n","instructions_datasets['test'] = instructions_datasets['test'].map(remove_answer, batched=False)\n","\n","instructions_datasets.set_format(\"torch\")\n","\n","instructions_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(instructions_datasets['validation'][1]['text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(instructions_datasets['validation'][1]['text_wa_answer'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Accessing the train, validation, and test splits\n","validation_dataset = instructions_datasets[\"validation\"]\n","test_dataset = instructions_datasets[\"test\"]\n","dev_dataset = instructions_datasets[\"dev\"]  # dataset for few shot\n","auxiliary_train_dataset  = instructions_datasets['auxiliary_train']\n","\n","# Check the size of each split\n","print(f\"Validation dataset size: {len(validation_dataset)}\")\n","print(f\"Test dataset size: {len(test_dataset)}\")\n","print(f\"Dev dataset size: {len(dev_dataset)}\")\n","print(f\"Auxiliary train dataset size: {len(auxiliary_train_dataset)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = AutoModelForCausalLM.from_pretrained(\n","    config.model_name,\n","    device_map='auto',\n","    torch_dtype=torch.float16,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["accuracy_metric = evaluate.load(\"accuracy\")\n","\n","def process_prediction(pred):\n","    pred = pred['generated_text']\n","    \n","    pred = pred.strip().upper()\n","    \n","    pred = pred[0] if pred else 'I'\n","    pred = pred if pred in {'A', 'B', 'C', 'D'} else 'I'\n","    \n","    return pred\n","\n","def compute_accuracy(model_preds, labels):   \n","    model_preds = list(map(process_prediction, model_preds))\n","    \n","    model_preds  = torch.LongTensor(list(map(ord, model_preds)))\n","    actual_labels = ord('A') + labels\n","    incorrect_labels = actual_labels.new_full(actual_labels.shape, ord('I'))\n","    \n","#     print(f\"{model_preds=}\\n{actual_labels=}\\n{incorrect_labels=}\")\n","    \n","    acc_res = accuracy_metric.compute(predictions=model_preds, references=actual_labels)['accuracy']\n","    corr_res = 1.0 - accuracy_metric.compute(predictions=model_preds, references=incorrect_labels)['accuracy']\n","    \n","    return {'accuracy': acc_res, 'correctness': corr_res}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lora_config = LoraConfig(\n","    task_type=TaskType.CAUSAL_LM,\n","    inference_mode=False, \n","    **OmegaConf.to_object(config.LoRA_config),\n",")\n","\n","lora_model = get_peft_model(model, lora_config)\n","lora_model.print_trainable_parameters()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.eval()\n","\n","pl = pipeline(\n","    \"text-generation\",\n","    model=model,       # WARNING: model used insted of lora_model\n","    tokenizer=tokenizer,\n","    torch_dtype=torch.float16\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","\n","model_preds = []\n","\n","with torch.no_grad():\n","    for i, split in tqdm.tqdm(\n","        enumerate(np.array_split(np.arange(len(test_dataset)), 20)),\n","        total=20,\n","    ):\n","        print(f\"Start iteration {i}\")\n","        print(f\"\\tstart pos: {np.min(split)}\\tend pos: {np.max(split)}\")\n","        \n","        model_pred = pl(\n","            test_dataset.select(split)['text_wa_answer'],\n","    #         validation_dataset.shuffle(42).select(range(512))['text_wa_answer'],\n","    #         validation_dataset['text_wa_answer'],\n","            return_full_text=False,\n","            max_new_tokens=4,\n","            do_sample=False,\n","            temperature=None,\n","            top_p=None,\n","            batch_size=1\n","        )\n","        model_preds += model_pred\n","        torch.cuda.empty_cache()\n","        \n","        print(f\"Finish iteration {i}\")\n","        print(f\"\\t{len(model_preds)=}\")\n","\n","model_preds_merged = []\n","for ls in model_preds:\n","    model_preds_merged += ls\n","\n","model_preds = model_preds_merged\n","\n","# model_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for i in range(len(model_preds)):\n","    model_preds[i]['subject'] = test_dataset[i]['subject']\n","    \n","model_preds "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with open('./fs_preds.bin', 'wb') as f:\n","    pickle.dump(\n","        obj=(model_preds, test_dataset['answer']),\n","        file=f\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["preds_df = pd.DataFrame(model_preds)\n","\n","preds_df['pred'] = preds_df.apply(process_prediction, axis=1)\n","preds_df['true'] = list(map(lambda v: chr(v + ord('A')), test_dataset['answer']))\n","preds_df['corr'] = (preds_df['pred'] == preds_df['true']).astype(np.int32)\n","preds_df['category'] = preds_df['subject'].apply(subcat_to_cat)\n","\n","preds_df.head(20)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["preds_df[['subject', 'corr']].groupby(['subject']).mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["preds_df[['category', 'corr']].groupby(['category']).mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["compute_accuracy(model_preds, test_dataset['answer'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["assert False"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["training_args = SFTConfig(\n","    **OmegaConf.to_object(config.trainer_config),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer = SFTTrainer(\n","    model=lora_model,\n","    args=training_args,\n","#     args=SFTConfig(\n","#         output_dir=\"/tmp\",\n","#         per_device_train_batch_size=1,\n","#         per_device_eval_batch_size=2,\n","#         fp16=True,\n","#     ),\n","    train_dataset=auxiliary_train_dataset,\n","    eval_dataset=validation_dataset.shuffle(42).select(range(64)),\n","#     formatting_func=formatting_prompts_func,\n","#     data_collator=collator,\n","#     compute_metrics=compute_accuracy,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# tokenizer.decode(trainer.train_dataset[0]['input_ids'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# for batch in trainer.get_train_dataloader():\n","#     print(tokenizer.batch_decode(batch['input_ids']))\n","#     break"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pl = pipeline(\n","    \"text-generation\",\n","    model=lora_model,\n","    tokenizer=tokenizer,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","\n","model_preds = pl(\n","    validation_dataset.shuffle(42).select(range(512))['text_wa_answer'],\n","#     validation_dataset['text_wa_answer'],\n","    return_full_text=False,\n","    max_new_tokens=16,\n","    do_sample=False,\n","    temperature=None,\n","    top_p=None,\n","    batch_size=4,\n",")\n","torch.cuda.empty_cache()\n","\n","model_preds_merged = []\n","for ls in model_preds:\n","    model_preds_merged += ls\n","\n","model_preds = model_preds_merged\n","\n","# model_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for i in range(len(model_preds)):\n","    model_preds[i]['subject'] = validation_dataset[i]['subject']\n","    \n","model_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["compute_accuracy(model_preds, validation_dataset.select(range(len(model_preds)))['answer'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lora_model.save_pretrained(\"./fine_tuned_model\")\n","tokenizer.save_pretrained(\"./fine_tuned_model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!zip fine_tuned_model.zip ./fine_tuned_model/*"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
