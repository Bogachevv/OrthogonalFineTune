{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import clear_output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers peft datasets evaluate huggingface_hub trl omegaconf rouge_score --upgrade\nclear_output()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport os\napi_keys = UserSecretsClient()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.system(f'wandb login {api_keys.get_secret(\"wandb\")}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.system(f'huggingface-cli login --token {api_keys.get_secret(\"huggingface-cli\")}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport scipy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM, AutoModelForSeq2SeqLM, BertTokenizer, BertModel \nfrom transformers import TrainingArguments, Trainer, Seq2SeqTrainingArguments, Seq2SeqTrainer, GenerationConfig, DataCollatorWithPadding\nfrom transformers import pipeline\nfrom peft import BOFTConfig, get_peft_model, LoraConfig, TaskType\nfrom datasets import load_dataset\nimport evaluate\nfrom trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n\nimport wandb\nfrom omegaconf import OmegaConf\n\n# from deepeval.benchmarks import MMLU\n# from deepeval.benchmarks.tasks import MMLUTask\n# from deepeval.models.base_model import DeepEvalBaseLLM\n\nimport pickle\nimport tqdm.notebook as tqdm\n\nclear_output()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Massive multitask language understanding (MMLU benchmark)","metadata":{}},{"cell_type":"code","source":"# Loading MMLU categories\nif not os.path.exists('./categories.py'):\n    !wget https://raw.githubusercontent.com/hendrycks/test/master/categories.py\n\nfrom categories import subcategories, categories as categories_inv","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for subcat_name, cat_names in subcategories.items():\n    subcategories[subcat_name] = cat_names[0] if isinstance(cat_names, list) else cat_names\n    \ncategories = {}\n\nfor cat_name, subcats in categories_inv.items():\n    for subcat in subcats:\n        categories[subcat] = cat_name","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def subcat_to_cat(subcat):\n    cat_name = subcategories[subcat]\n    cat_name = categories[cat_name]\n    \n    return cat_name","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = OmegaConf.create({\n    'model_name':   'meta-llama/Meta-Llama-3-8B-Instruct',\n    'padding_side': 'left',\n    'task_name':    'all',\n    'max_length':   256,\n    'n_shots': 2,\n    'fp16': True,\n    'bf16': False,\n    'ft_strategy': 'BOFT',\n    'LoRA_config': {\n        'r': 16, \n        'lora_alpha': 32, \n        'lora_dropout': 0.05,\n        'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n    },\n    'BOFT_config': {  # m=2, b=8\n        'boft_block_size': 8,\n#         'boft_block_num': 8,\n        'boft_n_butterfly_factor': 1,\n        'bias': 'none',\n        'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n        'boft_dropout': 0.05,\n    },\n    'evaluation_config':{\n        'num_splits': 20,\n        'max_new_tokens': 4,\n        'batch_size': 1,\n        'empty_cache': True,\n    },\n    'trainer_config': {\n        'output_dir': \"bogachevv/Llama-3-8b-MMLU\",\n        'max_seq_length': 512,\n        'dataset_text_field': 'text',\n        'fp16': True,\n        'full_determinism': False,\n        'per_device_train_batch_size': 1,\n        'per_device_eval_batch_size':  1,\n        'gradient_accumulation_steps': 8,\n        'lr_scheduler_type': 'cosine_with_restarts',\n        'lr_scheduler_kwargs':{\n            'num_cycles': 6,\n        },\n        'warmup_steps': 100,\n#         'num_train_epochs': 2,\n        'learning_rate': 1e-4,\n        'max_steps': 2048,\n        'weight_decay': 0.01,\n#         'warmup_ratio': 1e-2,\n        'dataloader_num_workers': 2,\n        'eval_strategy': \"steps\",\n#         'torch_empty_cache_steps': 16,\n        'eval_steps': 16,\n        'logging_steps': 16,\n        'load_best_model_at_end': True,\n        'seed': 42,\n        'data_seed': 42,\n        'report_to': 'wandb',\n#         'predict_with_generate': True,\n#         'push_to_hub': True,\n#         'hub_model_id': 'LLama-LoRA-test',\n#         'hub_strategy': 'checkpoint',\n#         'save_strategy': \"steps\",\n        'save_steps': 128,\n    },\n})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    config.model_name, \n    padding_side=config.padding_side,\n#     model_max_length=512,\n)\ntokenizer.pad_token = tokenizer.eos_token\nEOS_TOKEN = tokenizer.eos_token\n\nmmlu_dataset =  load_dataset(\"cais/mmlu\", config.task_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Try block for clear_output() call iff succes\ntry:\n    few_shot_datasets = {\n        subject: mmlu_dataset['dev'].filter(lambda row: row['subject'] == subject)\n        for subject in set(mmlu_dataset['dev']['subject'])\n    }\n    \n    clear_output()\n    print('Succes')\nexcept:\n    raise","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_question(examples):\n    prompt = f\"{examples['question']}\\n\"\n    for letter, choice in zip(('A', 'B', 'C', 'D'), examples['choices']):\n        prompt += f\"{letter}. {choice}\\n\"\n\n    answer = chr(65 + examples['answer'])\n    \n    return prompt, answer\n\ndef prepare_prompt(examples, dev_dataset = None):\n    if dev_dataset:\n        yield from map(prepare_question, dev_dataset)\n    \n    yield prepare_question(examples)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_instruction_text(example):\n    instructions = [\n        {\"role\": \"system\", \"content\": f\"The following are multiple choice questions (with answers) about {example['subject']}. Output 'A', 'B', 'C', or 'D'. Full answer not needed.\"},\n    ]\n\n    if config['n_shots'] and example['subject']:\n        few_shot_dataset = few_shot_datasets[example['subject']]\n        few_shot_dataset = few_shot_dataset.select(range(config['n_shots']))\n    else:\n        few_shot_dataset = None\n    \n    for prompt, ans in prepare_prompt(example, dev_dataset=few_shot_dataset):\n        instructions.append({\"role\": \"user\", \"content\": prompt})\n        instructions.append({\"role\": \"assistant\", \"content\": ans})\n    \n    text = tokenizer.apply_chat_template(\n        instructions,\n        tokenize=False\n    )\n    \n    return {'text': text}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def r_replace(line, old, new):\n    return line[::-1].replace(old[::-1], new[::-1], 1)[::-1]\n\ndef remove_answer(example):\n    text_wa_answer = example['text']\n    text_wa_answer = text_wa_answer.rsplit('<|eot_id|>', 1)[0][:-1]\n    \n    # for letter in ('A', 'B', 'C', 'D'):\n        # text_wa_answer = text_wa_answer.replace(f'<|start_header_id|>assistant<|end_header_id|>\\n\\n{letter}<|eot_id|>', '<|start_header_id|>assistant<|end_header_id|>\\n\\n')\n        # text_wa_answer = r_replace(text_wa_answer, f'<|start_header_id|>assistant<|end_header_id|>\\n\\n{letter}<|eot_id|>', '<|start_header_id|>assistant<|end_header_id|>\\n\\n')\n    \n    return {'text_wa_answer': text_wa_answer}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"instructions_datasets = mmlu_dataset.map(prepare_instruction_text, batched=False, num_proc=2)\ninstructions_datasets['validation'] = instructions_datasets['validation'].map(remove_answer, batched=False)\ninstructions_datasets['test'] = instructions_datasets['test'].map(remove_answer, batched=False)\n\ninstructions_datasets.set_format(\"torch\")\n\ninstructions_datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(instructions_datasets['validation'][1]['text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(instructions_datasets['validation'][1]['text_wa_answer'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Accessing the train, validation, and test splits\nvalidation_dataset = instructions_datasets[\"validation\"]\ntest_dataset = instructions_datasets[\"test\"]\ndev_dataset = instructions_datasets[\"dev\"]  # dataset for few shot\nauxiliary_train_dataset  = instructions_datasets['auxiliary_train']\n\n# Check the size of each split\nprint(f\"Validation dataset size: {len(validation_dataset)}\")\nprint(f\"Test dataset size: {len(test_dataset)}\")\nprint(f\"Dev dataset size: {len(dev_dataset)}\")\nprint(f\"Auxiliary train dataset size: {len(auxiliary_train_dataset)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    config.model_name,\n    device_map='auto',\n    torch_dtype=torch.float16,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_metric = evaluate.load(\"accuracy\")\n\ndef process_prediction(pred):\n    pred = pred['generated_text']\n    \n    pred = pred.strip().upper()\n    \n    pred = pred[0] if pred else 'I'\n    pred = pred if pred in {'A', 'B', 'C', 'D'} else 'I'\n    \n    return pred\n\ndef compute_accuracy(model_preds, labels):   \n    model_preds = list(map(process_prediction, model_preds))\n    \n    model_preds  = torch.LongTensor(list(map(ord, model_preds)))\n    actual_labels = ord('A') + labels\n    incorrect_labels = actual_labels.new_full(actual_labels.shape, ord('I'))\n    \n#     print(f\"{model_preds=}\\n{actual_labels=}\\n{incorrect_labels=}\")\n    \n    acc_res = accuracy_metric.compute(predictions=model_preds, references=actual_labels)['accuracy']\n    corr_res = 1.0 - accuracy_metric.compute(predictions=model_preds, references=incorrect_labels)['accuracy']\n    \n    return {'accuracy': acc_res, 'correctness': corr_res}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.model.layers[0].self_attn.q_proj.weight.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adapter_config = BOFTConfig(\n#     task_type=TaskType.CAUSAL_LM,\n#     inference_mode=False,\n#     boft_block_size=8,\n# #     boft_block_num=16,\n#     boft_n_butterfly_factor=2,\n#     bias='none',\n# )\n\n# adapter_config\n\n# model_adapter = get_peft_model(model, adapter_config)    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_adapter.print_trainable_parameters()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_adapter","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adapter_config = BOFTConfig(\n        task_type=TaskType.CAUSAL_LM,\n        inference_mode=False,\n        **OmegaConf.to_object(config.BOFT_config)\n    )\n\nmodel_adapter = get_peft_model(model, adapter_config)   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if config.ft_strategy == 'LoRA':\n    adapter_config = LoraConfig(\n        task_type=TaskType.CAUSAL_LM,\n        inference_mode=False, \n        **OmegaConf.to_object(config.LoRA_config),\n    )\nelif config.ft_strategy == 'BOFT':\n    adapter_config = BOFTConfig(\n        task_type=TaskType.CAUSAL_LM,\n        inference_mode=False,\n        **OmegaConf.to_object(config.BOFT_config)\n    )\nelse:\n    raise ValueError('Incorrect FT type')\n\nmodel_adapter = get_peft_model(model, adapter_config)    \nmodel_adapter.print_trainable_parameters()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\npl = pipeline(\n    \"text-generation\",\n    model=model,       # WARNING: model used insted of lora_model\n    tokenizer=tokenizer,\n    torch_dtype=torch.float16\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel_preds = []\n\nwith torch.no_grad():\n    for i, split in tqdm.tqdm(\n        enumerate(np.array_split(np.arange(len(test_dataset)), 20)),\n        total=20,\n    ):\n        print(f\"Start iteration {i}\")\n        print(f\"\\tstart pos: {np.min(split)}\\tend pos: {np.max(split)}\")\n        \n        model_pred = pl(\n            test_dataset.select(split)['text_wa_answer'],\n    #         validation_dataset.shuffle(42).select(range(512))['text_wa_answer'],\n    #         validation_dataset['text_wa_answer'],\n            return_full_text=False,\n            max_new_tokens=4,\n            do_sample=False,\n            temperature=None,\n            top_p=None,\n            batch_size=1\n        )\n        model_preds += model_pred\n        torch.cuda.empty_cache()\n        \n        print(f\"Finish iteration {i}\")\n        print(f\"\\t{len(model_preds)=}\")\n\nmodel_preds_merged = []\nfor ls in model_preds:\n    model_preds_merged += ls\n\nmodel_preds = model_preds_merged\n\n# model_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(model_preds)):\n    model_preds[i]['subject'] = test_dataset[i]['subject']\n    \nmodel_preds ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('./fs_preds.bin', 'wb') as f:\n    pickle.dump(\n        obj=(model_preds, test_dataset['answer']),\n        file=f\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_df = pd.DataFrame(model_preds)\n\npreds_df['pred'] = preds_df.apply(process_prediction, axis=1)\npreds_df['true'] = list(map(lambda v: chr(v + ord('A')), test_dataset['answer']))\npreds_df['corr'] = (preds_df['pred'] == preds_df['true']).astype(np.int32)\npreds_df['category'] = preds_df['subject'].apply(subcat_to_cat)\n\npreds_df.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_df[['subject', 'corr']].groupby(['subject']).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_df[['category', 'corr']].groupby(['category']).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compute_accuracy(model_preds, test_dataset['answer'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = SFTConfig(\n    **OmegaConf.to_object(config.trainer_config),\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=lora_model,\n    args=training_args,\n#     args=SFTConfig(\n#         output_dir=\"/tmp\",\n#         per_device_train_batch_size=1,\n#         per_device_eval_batch_size=2,\n#         fp16=True,\n#     ),\n    train_dataset=auxiliary_train_dataset,\n    eval_dataset=validation_dataset.shuffle(42).select(range(64)),\n#     formatting_func=formatting_prompts_func,\n#     data_collator=collator,\n#     compute_metrics=compute_accuracy,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenizer.decode(trainer.train_dataset[0]['input_ids'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for batch in trainer.get_train_dataloader():\n#     print(tokenizer.batch_decode(batch['input_ids']))\n#     break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\n\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pl = pipeline(\n    \"text-generation\",\n    model=lora_model,\n    tokenizer=tokenizer,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel_preds = pl(\n    validation_dataset.shuffle(42).select(range(512))['text_wa_answer'],\n#     validation_dataset['text_wa_answer'],\n    return_full_text=False,\n    max_new_tokens=16,\n    do_sample=False,\n    temperature=None,\n    top_p=None,\n    batch_size=4,\n)\ntorch.cuda.empty_cache()\n\nmodel_preds_merged = []\nfor ls in model_preds:\n    model_preds_merged += ls\n\nmodel_preds = model_preds_merged\n\n# model_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(model_preds)):\n    model_preds[i]['subject'] = validation_dataset[i]['subject']\n    \nmodel_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compute_accuracy(model_preds, validation_dataset.select(range(len(model_preds)))['answer'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lora_model.save_pretrained(\"./fine_tuned_model\")\ntokenizer.save_pretrained(\"./fine_tuned_model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip fine_tuned_model.zip ./fine_tuned_model/*","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}